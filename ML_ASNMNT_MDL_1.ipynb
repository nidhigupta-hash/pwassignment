{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "QNS-1-What is a parameter.\n",
        "\n",
        "ANS-\n",
        "In machine learning (ML), a parameter is a numerical value that is used to configure a model or algorithm. Parameters are typically learned from the training data during the model training process.\n"
      ],
      "metadata": {
        "id": "Iu1J0X6Vg6ca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-2-What is correlation.\n",
        "\n",
        "ANS-\n",
        "Correlation is a statistical measure that describes the relationship between two continuous variables. It measures how strongly two variables are related to each other, and whether this relationship is positive, negative, or neutral.\n",
        "\n",
        "Types of Correlation:\n",
        "\n",
        "1. Positive Correlation\n",
        "2. Negative Correlation\n",
        "3. Neutral Correlation\n",
        "\n",
        "Negative Correlation:\n",
        "\n",
        " When two variables tend to move in opposite directions, the correlation is negative. For example, the relationship between the amount of rain and the number of people visiting the beach.\n",
        "\n"
      ],
      "metadata": {
        "id": "U8cowTALhTR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-3-Define Machine Learning. What are the main components in Machine Learning\n",
        "\n",
        "ANS-\n",
        "\n",
        "Definition of Machine Learning:\n",
        "\n",
        "Machine learning (ML) is a subset of artificial intelligence (AI) that involves the use of algorithms and statistical models to enable machines to learn from data, make decisions, and improve their performance on a task without being explicitly programmed.\n",
        "\n",
        "Main Components of Machine Learning:\n",
        "\n",
        "1. Data: Machine learning relies heavily on data. The quality and quantity of data used to train a model can significantly impact its performance.\n",
        "2. Algorithms: Machine learning algorithms are the core of any ML system. These algorithms enable the model to learn from the data and make predictions or decisions.\n",
        "3. Model: A machine learning model is the result of training an algorithm on a dataset. The model can be used to make predictions or decisions on new, unseen data.\n",
        "4. Features: Features are the individual characteristics of the data that are used to train the model. For example, in image classification, features might include color, texture, and shape.\n",
        "5. Target Variable: The target variable is the variable that the model is trying to predict. For example, in a classification problem, the target variable might be the class label.\n",
        "\n"
      ],
      "metadata": {
        "id": "WrAEqJhHhiCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-4-How does loss value help in determining whether the model is good or not\n",
        "\n",
        "ANS-\n",
        "\n",
        "How Loss Value Helps in Determining Model Performance:\n",
        "\n",
        "1. Lower Loss Value: A lower loss value indicates that the model is making accurate predictions and is closer to the true values.\n",
        "2. Higher Loss Value: A higher loss value indicates that the model is making inaccurate predictions and is farther away from the true values.\n",
        "3. Convergence: A decreasing loss value over time indicates that the model is converging to a optimal solution.\n",
        "4. Overfitting: A low loss value on the training data but a high loss value on the validation data may indicate overfitting.\n",
        "5. Underfitting: A high loss value on both the training and validation data may indicate underfitting."
      ],
      "metadata": {
        "id": "POrYJW-DiGEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-5-What are continuous and categorical variables\n",
        "\n",
        "ANS-\n",
        "\n",
        "Continuous Variables:\n",
        "\n",
        "Continuous variables are variables that can take any value within a given range or interval. They can be measured to any level of precision and are often represented by numbers. Examples of continuous variables include:\n",
        "\n",
        "- Age\n",
        "- Height\n",
        "- Weight\n",
        "- Temperature\n",
        "- Time\n",
        "\n",
        "Categorical Variables:\n",
        "\n",
        "Categorical variables, also known as nominal or discrete variables, are variables that can only take on a limited number of distinct values or categories. These variables are often represented by words, labels, or numbers that are used to identify the categories. Examples of categorical variables include:\n",
        "\n",
        "- Color (red, blue, green, etc.)\n",
        "- Sex (male, female, etc.)\n",
        "- Marital status (married, single, divorced, etc.)\n",
        "- Occupation (student, teacher, engineer, etc.)\n",
        "- Species (human, animal, plant, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "xbL6zB5wiUa7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-6-How do we handle categorical variables in Machine Learning? What are the common t echniques.\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here are some common techniques for handling categorical variables:\n",
        "\n",
        "1. Label Encoding: This involves assigning a unique numerical value to each category. For example, if we have a categorical variable \"color\" with categories \"red\", \"blue\", and \"green\", we could assign the values 0, 1, and 2 to each category, respectively.\n",
        "2. One-Hot Encoding (OHE): This involves creating a new binary variable for each category. For example, if we have a categorical variable \"color\" with categories \"red\", \"blue\", and \"green\", we could create three new binary variables: \"is_red\", \"is_blue\", and \"is_green\".\n",
        "3. Binary Encoding: This involves representing each category as a binary vector. For example, if we have a categorical variable \"color\" with categories \"red\", \"blue\", and \"green\", we could represent each category as a binary vector: \"red\" = [1, 0, 0], \"blue\" = [0, 1, 0], and \"green\" = [0, 0, 1].\n",
        "4. Hashing: This involves using a hash function to map each category to a numerical value. This can be useful when working with high-cardinality categorical variables.\n",
        "5. Embeddings: This involves learning a dense representation of each category using techniques such as word2vec or glove. This can be useful when working with categorical variables that have a large number of categories.\n",
        "\n"
      ],
      "metadata": {
        "id": "aI637UD8it4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-7-What do you mean by training and testing a dataset.\n",
        "\n",
        "ANS-\n",
        "\n",
        "\n",
        "Training Set:\n",
        "\n",
        "The training set is used to train a machine learning model. The model learns to recognize patterns and relationships in the data by optimizing its parameters to minimize the error between its predictions and the actual labels.\n",
        "\n",
        "Testing Set:\n",
        "\n",
        "The testing set, also known as the validation set or holdout set, is used to evaluate the performance of the trained model. The model is applied to the testing set, and its predictions are compared to the actual labels. This provides an unbiased estimate of the model's performance on unseen data.\n"
      ],
      "metadata": {
        "id": "e17tFXkZi7z4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-8-What is sklearn.preprocessing.\n",
        "\n",
        "ANS-\n",
        "\n",
        "sklearn.preprocessing is a module in scikit-learn, a popular Python machine learning library. This module provides various functions and classes for preprocessing data, which is an essential step in machine learning pipelines.\n"
      ],
      "metadata": {
        "id": "wHgrkCOQjgD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-9-What is a Test set\n",
        "\n",
        "ANS-\n",
        "\n",
        "A test set, also known as a holdout set or evaluation set, is a portion of a dataset that is used to evaluate the performance of a machine learning model after it has been trained. The test set is typically a separate subset of data that is not used during the training process.\n",
        "\n",
        "Purpose of a Test Set:\n",
        "\n",
        "The primary purpose of a test set is to provide an unbiased evaluation of a machine learning model's performance on unseen data. By using a test set, you can:\n",
        "\n",
        "1. Evaluate Model Performance: Assess the model's accuracy, precision, recall, F1-score, and other relevant metrics.\n",
        "2. Avoid Overfitting: Detect whether the model is overfitting to the training data and not generalizing well to new, unseen data.\n",
        "3. Compare Models: Compare the performance of different models or algorithms on the same test set.\n",
        "4. Hyperparameter Tuning: Use the test set to evaluate the performance of a model with different hyperparameters and select the best combination.\n"
      ],
      "metadata": {
        "id": "yljnHFhwjvGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##QSN-10-How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "#ANS-\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset into a pandas DataFrame\n",
        "df = pd.read_csv('plant.csv')\n",
        "\n",
        "# Split your data into features (X) and target variable (y)\n",
        "X = df.drop('target_variable', axis=1)\n",
        "y = df['target_variable']\n",
        "\n",
        "# Split your data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "2Vogw2sSkLhj",
        "outputId": "553197be-ce9b-479a-d8b6-d1a2f73df03f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'plant.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5a227a0a712e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load your dataset into a pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'plant.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Split your data into features (X) and target variable (y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'plant.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-11-Why do we have to perform EDA before fitting a model to the data.\n",
        "ANS-\n",
        "\n",
        "Performing Exploratory Data Analysis (EDA) before fitting a model to the data is essential for several reasons:\n",
        "\n",
        "1. Understanding Data Distribution: EDA helps you understand the distribution of your data, including the shape, central tendency, and variability. This information is crucial for selecting the right model and making informed decisions.\n",
        "2. Identifying Outliers and Anomalies: EDA helps you detect outliers and anomalies in your data, which can significantly impact model performance. By identifying and addressing these issues, you can improve the accuracy and reliability of your model.\n",
        "3. Discovering Relationships and Correlations: EDA helps you discover relationships and correlations between variables, which can inform feature engineering and model selection.\n",
        "4. Checking Assumptions: EDA helps you check the assumptions of various statistical models, such as normality, linearity, and homoscedasticity. By verifying these assumptions, you can ensure that your model is appropriate for the data."
      ],
      "metadata": {
        "id": "PRHS30CGk97X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-12-What is correlation\n",
        "\n",
        "ANS-\n",
        "\n",
        "Correlation is a statistical measure that describes the relationship between two continuous variables. It measures how strongly two variables are related to each other, and whether this relationship is positive, negative, or neutral.\n",
        "\n",
        "Types of Correlation:\n",
        "\n",
        "Positive Correlation\n",
        "Negative Correlation\n",
        "Neutral Correlation\n"
      ],
      "metadata": {
        "id": "EF33cJoKlMny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-13-What does negative correlation mean.\n",
        "\n",
        "ANS-\n",
        "\n",
        "Negative Correlation:\n",
        "\n",
        "When two variables tend to move in opposite directions, the correlation is negative. For example, the relationship between the amount of rain and the number of people visiting the beach."
      ],
      "metadata": {
        "id": "uggBiPvmlcfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QSN-14-How can you find correlation between variables in Python.\n",
        "#ANS-\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a sample DataFrame\n",
        "data = {'A': [1, 2, 3, 4, 5],\n",
        "        'B': [2, 3, 5, 7, 11],\n",
        "        'C': [3, 4, 6, 8, 12]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Print the correlation matrix\n",
        "print(corr_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1TCJTZkl-1e",
        "outputId": "ede22251-d58d-4257-9ff3-9d885473ccb4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          A         B         C\n",
            "A  1.000000  0.972272  0.972272\n",
            "B  0.972272  1.000000  1.000000\n",
            "C  0.972272  1.000000  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-15-What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "ANS-\n",
        "\n",
        "Causation refers to the relationship between two events or variables where one event (the cause) leads to the occurrence of the other event (the effect). In other words, causation implies that one variable has a direct influence on the other variable.\n",
        "\n",
        "Correlation vs. Causation:\n",
        "\n",
        "Correlation and causation are often confused with each other, but they are not the same thing. Correlation refers to the statistical relationship between two variables, whereas causation implies a direct cause-and-effect relationship.\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose we observe a correlation between the number of ice cream sales and the number of people wearing shorts in a given city. We might find that on hot days, both ice cream sales and the number of people wearing shorts tend to increase.\n",
        "\n",
        "- Correlation: We can say that there is a correlation between ice cream sales and people wearing shorts. However, this correlation does not necessarily imply causation.\n",
        "- Causation: In this case, the causation is likely due to a third variable: the temperature. On hot days, people are more likely to buy ice cream and wear shorts. The temperature is the underlying cause of both phenomena, but there is no direct causal relationship between ice cream sales and people wearing shorts.\n"
      ],
      "metadata": {
        "id": "rj4Ms51vmZua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-16-What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "ANS-\n",
        "\n",
        "An optimizer is an algorithm used in machine learning to minimize or maximize a loss function or objective function. The optimizer's goal is to adjust the model's parameters to achieve the best possible performance on a given task.\n",
        "\n",
        "Types of Optimizers:\n",
        "\n",
        "Here are some common types of optimizers, along with examples:\n",
        "\n",
        "1. Gradient Descent (GD)\n",
        "\n",
        "Gradient Descent is an optimization algorithm that iteratively adjusts the model's parameters to minimize the loss function. It uses the gradient of the loss function to determine the direction of the update.\n",
        "\n",
        "Example: Suppose we want to minimize the function f(x) = x^2. We start with an initial guess x=2. The gradient of f(x) is 2x. We update x using the gradient descent update rule: x_new = x_old - learning_rate * gradient. If the learning rate is 0.1, the update would be: x_new = 2 - 0.1 * 2 * 2 = 1.6.\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "An optimizer is an algorithm used in machine learning to minimize or maximize a loss function or objective function. The optimizer's goal is to adjust the model's parameters to achieve the best possible performance on a given task.\n",
        "\n",
        "Types of Optimizers:\n",
        "\n",
        "Here are some common types of optimizers, along with examples:\n",
        "\n",
        "1. Gradient Descent (GD)\n",
        "\n",
        "Gradient Descent is an optimization algorithm that iteratively adjusts the model's parameters to minimize the loss function. It uses the gradient of the loss function to determine the direction of the update.\n",
        "\n",
        "Example: Suppose we want to minimize the function f(x) = x^2. We start with an initial guess x=2. The gradient of f(x) is 2x. We update x using the gradient descent update rule: x_new = x_old - learning_rate * gradient. If the learning rate is 0.1, the update would be: x_new = 2 - 0.1 * 2 * 2 = 1.6.\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "An optimizer is an algorithm used in machine learning to minimize or maximize a loss function or objective function. The optimizer's goal is to adjust the model's parameters to achieve the best possible performance on a given task.\n",
        "\n",
        "Types of Optimizers:\n",
        "\n",
        "Here are some common types of optimizers, along with examples:\n",
        "\n",
        "1. Gradient Descent (GD)\n",
        "\n",
        "Gradient Descent is an optimization algorithm that iteratively adjusts the model's parameters to minimize the loss function. It uses the gradient of the loss function to determine the direction of the update.\n",
        "\n",
        "Example: Suppose we want to minimize the function f(x) = x^2. We start with an initial guess x=2. The gradient of f(x) is 2x. We update x using the gradient descent update rule: x_new = x_old - learning_rate * gradient. If the learning rate is 0.1, the update would be: x_new = 2 - 0.1 * 2 * 2 = 1.6.\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Stochastic Gradient Descent is a variant of Gradient Descent that uses a single example from the training dataset to compute the gradient, rather than the entire dataset.\n",
        "\n",
        "Example: Suppose we want to train a linear regression model on a dataset of exam scores and hours studied. We start with an initial guess for the model's parameters. We select a single example from the dataset, compute the gradient of the loss function, and update the model's parameters using the SGD update rule.\n",
        "\n",
        "3. Momentum\n",
        "\n",
        "Momentum is an optimization algorithm that adds a fraction of the previous update to the current update, to help escape local minima.\n",
        "\n",
        "Example: Suppose we want to minimize the function f(x) = x^2 + 0.1 * sin(10x). We start with an initial guess x=2. We use the momentum update rule: v_new = gamma * v_old + learning_rate * gradient, where v is the velocity. If the learning rate is 0.1, gamma is 0.9, and the gradient is 2x, the update would be: v_new = 0.9 * v_old + 0.1 * 2 * 2.\n",
        "\n",
        "4. Nesterov Accelerated Gradient (NAG)\n",
        "\n",
        "Nesterov Accelerated Gradient is an optimization algorithm that uses the gradient of the loss function at the current point, as well as the gradient at the point where the current point would be if the current velocity were zero.\n",
        "\n",
        "Example: Suppose we want to minimize the function f(x) = x^2 + 0.1 * sin(10x). We start with an initial guess x=2. We use the NAG update rule: v_new = gamma * v_old + learning_rate * gradient, where v is the velocity. If the learning rate is 0.1, gamma is 0.9, and the gradient is 2x, the update would be: v_new = 0.9 * v_old + 0.1 * 2 * 2"
      ],
      "metadata": {
        "id": "t8S2szPxmikd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-17-What is sklearn.linear_model.\n",
        "\n",
        "ANS-\n",
        "\n",
        "\n",
        "sklearn.linear_model is a module in scikit-learn, a popular Python machine learning library, that provides classes and functions for linear models.\n",
        "\n",
        "Linear Models\n",
        "\n",
        "Linear models are a type of supervised learning algorithm that predict a continuous output variable based on one or more input features. The relationship between the input features and the output variable is modeled using a linear equation.\n",
        "\n",
        "Classes and Functions in sklearn.linear_model\n",
        "\n",
        "Some of the most commonly used classes and functions in sklearn.linear_model include:\n",
        "\n",
        "1. LinearRegression: A class for linear regression, which predicts a continuous output variable based on one or more input features.\n",
        "2. Ridge: A class for ridge regression, which is a type of linear regression that includes an L2 penalty term to prevent overfitting.\n",
        "3. Lasso: A class for lasso regression, which is a type of linear regression that includes an L1 penalty term to prevent overfitting.\n",
        "4. ElasticNet: A class for elastic net regression, which is a type of linear regression that includes both L1 and L2 penalty terms to prevent overfitting.\n",
        "5. LogisticRegression: A class for logistic regression, which predicts a binary output variable based on one or more input features.\n"
      ],
      "metadata": {
        "id": "md3fAowfnDIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-18-What does model.fit() do? What arguments must be given\n",
        "\n",
        "ANS-\n",
        "model.fit() is a method in scikit-learn and Keras that trains a machine learning model on a given dataset. It takes in the training data and optimizes the model's parameters to minimize the loss function.\n",
        "\n",
        "Arguments:\n",
        "\n",
        "The fit() method typically takes in the following arguments:\n",
        "\n",
        "1. X: The feature data, which is a 2D array-like object with shape (n_samples, n_features).\n",
        "2. y: The target data, which is a 1D array-like object with shape (n_samples,).\n",
        "\n"
      ],
      "metadata": {
        "id": "6DiEWr2WRAK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-19-What does model.predict() do? What arguments must be given.\n",
        "\n",
        "ANS-\n",
        "model.predict() is a method in scikit-learn and Keras that uses a trained machine learning model to make predictions on new, unseen data.\n",
        "\n",
        "model.predict() is a method in scikit-learn and Keras that uses a trained machine learning model to make predictions on new, unseen data.\n",
        "\n",
        "Arguments:\n",
        "\n",
        "The predict() method typically takes in the following argument:\n",
        "\n",
        "1. X: The feature data to make predictions on, which is a 2D array-like object with shape (n_samples, n_features).\n",
        "\n",
        "Optional arguments:\n",
        "\n",
        "1. batch_size: The number of samples to include in a single batch. Default is 32.\n",
        "2. verbose: An integer indicating the verbosity level. 0 = silent, 1 = progress bar.\n",
        "3. steps: The number of steps to predict. Only applicable for sequence-to-sequence models.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6973Y92SRqp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-20-What are continuous and categorical variables.\n",
        "\n",
        "ANS-\n",
        "Continuous variables are variables that can take any value within a given range or interval. They can be measured to any level of precision and are often represented by numbers. Examples of continuous variables include:\n",
        "\n",
        "Age\n",
        "Height\n",
        "Weight\n",
        "Temperature\n",
        "Time\n",
        "Categorical Variables:\n",
        "\n",
        "Categorical variables, also known as nominal or discrete variables, are variables that can only take on a limited number of distinct values or categories. These variables are often represented by words, labels, or numbers that are used to identify the categories. Examples of categorical variables include:\n",
        "\n",
        "Color (red, blue, green, etc.)\n",
        "Sex (male, female, etc.)\n",
        "Marital status (married, single, divorced, etc.)\n",
        "Occupation (student, teacher, engineer, etc.)\n",
        "Species (human, animal, plant, etc.)"
      ],
      "metadata": {
        "id": "bvVfGwbASpad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-21-What is feature scaling? How does it help in Machine Learning.\n",
        "\n",
        "ANS-\n",
        "\n",
        "Feature scaling, also known as normalization or standardization, is a technique used in machine learning to rescale the features of a dataset to a common range, usually between 0 and 1. This is done to prevent features with large ranges from dominating the model and to improve the convergence of optimization algorithms.\n",
        "\n",
        "Why is Feature Scaling Important?\n",
        "Feature scaling is important for several reasons:\n",
        "\n",
        "1. Prevents Feature Dominance: When features have different scales, the feature with the largest scale can dominate the model, leading to poor performance. Feature scaling prevents this by giving all features equal importance.\n",
        "2. Improves Convergence: Many optimization algorithms, such as gradient descent, converge faster when features are scaled. This is because the gradients are more stable and the algorithm can take larger steps.\n",
        "3. Enhances Model Interpretability: Feature scaling can make model interpretation easier by ensuring that all features are on the same scale.\n"
      ],
      "metadata": {
        "id": "fY7lBwYeUnfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QSN-22-How do we perform scaling in Python.\n",
        "\n",
        "#ANS-\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a sample dataset\n",
        "data = {'Feature1': [1, 2, 3, 4, 5],\n",
        "        'Feature2': [10, 20, 30, 40, 50]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the data and transform it\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Convert the scaled data back to a DataFrame\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "\n",
        "print(scaled_df)\n",
        "\n",
        "##SECOND WAY\n",
        "import pandas as pd\n",
        "\n",
        "# Create a sample dataset\n",
        "data = {'Feature1': [1, 2, 3, 4, 5],\n",
        "        'Feature2': [10, 20, 30, 40, 50]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define a scaling function\n",
        "def scale(x):\n",
        "    return (x - x.min()) / (x.max() - x.min())\n",
        "\n",
        "# Apply the scaling function to each column\n",
        "scaled_df = df.apply(scale)\n",
        "\n",
        "print(scaled_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFnGzfOxVSTs",
        "outputId": "1c4780dc-4860-415c-d1cb-a1e5ce97e4fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Feature1  Feature2\n",
            "0 -1.414214 -1.414214\n",
            "1 -0.707107 -0.707107\n",
            "2  0.000000  0.000000\n",
            "3  0.707107  0.707107\n",
            "4  1.414214  1.414214\n",
            "   Feature1  Feature2\n",
            "0      0.00      0.00\n",
            "1      0.25      0.25\n",
            "2      0.50      0.50\n",
            "3      0.75      0.75\n",
            "4      1.00      1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QSN-23-What is sklearn.preprocessing.\n",
        "\n",
        "#ANS-sklearn.preprocessing is a module in scikit-learn, a popular Python machine learning library, that provides various functions and classes for preprocessing data.\n",
        "#The purpose of preprocessing is to transform raw data into a format that is more suitable for machine learning algorithms\n",
        "#EXAMPLE-\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Create a sample dataset\n",
        "data = {'Feature1': [1, 2, 3, 4, 5],\n",
        "        'Feature2': [10, 20, 30, 40, 50]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the data and transform it\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Convert the scaled data back to a DataFrame\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "\n",
        "print(scaled_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3smIEoHwWQB2",
        "outputId": "29da0379-d171-435e-8124-aedc0fc255d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Feature1  Feature2\n",
            "0 -1.414214 -1.414214\n",
            "1 -0.707107 -0.707107\n",
            "2  0.000000  0.000000\n",
            "3  0.707107  0.707107\n",
            "4  1.414214  1.414214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QSN-24-How do we split data for model fitting (training and testing) in Python.\n",
        "#ANS-\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "#Creating a Sample Dataset\n",
        "data = {'Feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "        'Feature2': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
        "        'Target': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = df[['Feature1', 'Feature2']]  # Features\n",
        "y = df['Target']  # Target variable\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Testing data shape:\", X_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO6fjVRcWr5h",
        "outputId": "afc19091-05fd-477f-e124-f22e4d8cbd16"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (8, 2) (8,)\n",
            "Testing data shape: (2, 2) (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSN-25-Explain data encoding.\n",
        "\n",
        "ANS-\n",
        "\n",
        "Data encoding is the process of converting categorical data into numerical data that can be processed by machine learning algorithms. This is necessary because many machine learning algorithms, such as linear regression and neural networks, require numerical input data.\n"
      ],
      "metadata": {
        "id": "y--ugHWIXXf5"
      }
    }
  ]
}